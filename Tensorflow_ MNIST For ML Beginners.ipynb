{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "#!pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "### Hello, Tensor World!\nLet\u2019s analyze the Hello World script you ran. For reference, I\u2019ve added the code below.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import tensorflow as tf\n\n# Create TensorFlow object called hello_constant\nhello_constant = tf.constant('Hello World!')\n\nwith tf.Session() as sess:\n    # Run the tf.constant operation in the session\n    output = sess.run(hello_constant)\n    print(output)", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Hello World!\n"
                }
            ], 
            "execution_count": 1
        }, 
        {
            "source": "### Tensor\n\nIn TensorFlow, data isn\u2019t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. In the case of ```hello_constant = tf.constant('Hello World!')```, ```hello_constant!')```, is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# A is a 0-dimensional int32 tensor\nA = tf.constant(1234) \n# B is a 1-dimensional int32 tensor\nB = tf.constant([ [123,456,789] ]) \n # C is a 2-dimensional int32 tensor\nC = tf.constant([ [123,456,789], [222,333,444] ])", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "### Session\n\nTensorFlow\u2019s api is built around the idea of a computational graph, a way of visualizing a mathematical process. Let\u2019s take the TensorFlow code you ran and turn that into a graph:\n<img src=\"https://d17h27t6h515a5.cloudfront.net/topher/2016/October/580feadb_session/session.png\" alt=\"\" class=\"index--image--1xyr4\" style=\"height: 312.419px; width: 539px;\">\nA \"TensorFlow Session\", as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let\u2019s see how you use it.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "with tf.Session() as sess:\n    output = sess.run(hello_constant)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "source": "The code has already created the tensor, ```hello_constant```, from the previous lines. The next step is to evaluate the tensor in a session.\n\nThe code creates a session instance, sess, using ```tf.Session```,. The ```sess.run()```, function then evaluates the tensor and returns the results.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# MNIST For ML Beginners\n\nThis tutorial is intended for readers who are new to both machine learning and TensorFlow. If you already know what MNIST is, and what softmax (multinomial logistic) regression is, you might prefer this faster paced tutorial. Be sure to install TensorFlow before starting either tutorial.\n\nWhen one learns how to program, there's a tradition that the first thing you do is print \"Hello World.\" Just like programming has Hello World, machine learning has MNIST.\n\nMNIST is a simple computer vision dataset. It consists of images of handwritten digits like these:\n<img style=\"width:100%\" src=\"https://www.tensorflow.org/images/MNIST.png\">", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "It also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.\n\nIn this tutorial, we're going to train a model to look at images and predict what digits they are. Our goal isn't to train a really elaborate model that achieves state-of-the-art performance -- although we'll give you code to do that later! -- but rather to dip a toe into using TensorFlow. As such, we're going to start with a very simple model, called a Softmax Regression.\n\nThe actual code for this tutorial is very short, and all the interesting stuff happens in just three lines. However, it is very important to understand the ideas behind it: both how TensorFlow works and the core machine learning concepts. Because of this, we are going to very carefully work through the code.\n\n### About this tutorial\n\nThis tutorial is an explanation, line by line, of what is happening in the mnist_softmax.py code.\n\nYou can use this tutorial in a few different ways, including:\n\n- Copy and paste each code snippet, line by line, into a Python environment as you read through the explanations of each line.\n- Run the entire mnist_softmax.py Python file either before or after reading through the explanations, and use this tutorial to understand the lines of code that aren't clear to you.\nWhat we will accomplish in this tutorial:\n\n- Learn about the MNIST data and softmax regressions\n- Create a function that is a model for recognizing digits, based on looking at every pixel in the image\n- Use Tensorflow to train the model to recognize digits by having it \"look\" at thousands of examples (and run our first Tensorflow session to do so)\n- Check the model's accuracy with our test data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#!pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0-cp27-none-linux_x86_64.whl", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "The MNIST data is hosted on Yann LeCun's website. If you are copying and pasting in the code from this tutorial, start here with these two lines of code which will download and read in the data automatically:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }, 
        {
            "source": "Import tensorflow library:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "import tensorflow as tf\n", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 2
        }, 
        {
            "source": "We describe these interacting operations by manipulating symbolic variables. Let's create one:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "x = tf.placeholder(tf.float32, [None, 784])\n", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 3
        }, 
        {
            "source": "x isn't a specific value. It's a placeholder, a value that we'll input when we ask TensorFlow to run a computation. We want to be able to input any number of MNIST images, each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of floating-point numbers, with a shape [None, 784]. (Here None means that a dimension can be of any length.)\n\nWe also need the weights and biases for our model. We could imagine treating these like additional inputs, but TensorFlow has an even better way to handle it: Variable. A Variable is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation. For machine learning applications, one generally has the model parameters be Variables.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "W = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 4
        }, 
        {
            "source": "We create these Variables by giving tf.Variable the initial value of the Variable: in this case, we initialize both W and b as tensors full of zeros. Since we are going to learn W and b, it doesn't matter very much what they initially are.\n\nNotice that W has a shape of [784, 10] because we want to multiply the 784-dimensional image vectors by it to produce 10-dimensional vectors of evidence for the difference classes. b has a shape of [10] so we can add it to the output.\n\nWe can now implement our model. It only takes one line to define it!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "y = tf.nn.softmax(tf.matmul(x, W) + b)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "### Training\n\nIn order to train our model, we need to define what it means for the model to be good. Well, actually, in machine learning we typically define what it means for a model to be bad. We call this the cost, or the loss, and it represents how far off our model is from our desired outcome. We try to minimize that error, and the smaller the error margin, the better our model is.\n\nTo implement cross-entropy we need to first add a new placeholder to input the correct answers:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "y_ = tf.placeholder(tf.float32, [None, 10])", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 5
        }, 
        {
            "source": "Then we can implement the cross-entropy function", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 7
        }, 
        {
            "source": "Now that we know what we want our model to do, it's very easy to have TensorFlow train it to do so. Because TensorFlow knows the entire graph of your computations, it can automatically use the backpropagation algorithm to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 8
        }, 
        {
            "source": "init = tf.initialize_all_variables()\n", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "execution_count": 10
        }, 
        {
            "source": "In this case, we ask TensorFlow to minimize cross_entropy using the gradient descent algorithm with a learning rate of 0.5. Gradient descent is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. But TensorFlow also provides many other optimization algorithms: using one is as simple as tweaking one line.\n\nWhat TensorFlow actually does here, behind the scenes, is to add new operations to your graph which implement backpropagation and gradient descent. Then it gives you back a single operation which, when run, does a step of gradient descent training, slightly tweaking your variables to reduce the loss.\n\nWe can now launch the model in an InteractiveSession:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "sess = tf.Session()\nsess.run(init)", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 11
        }, 
        {
            "source": "Let's train -- we'll run the training step 1000 times!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "for i in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 12
        }, 
        {
            "source": "### Evaluating Our Model\n\nHow well does our model do?\n\nWell, first let's figure out where we predicted the correct label. tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. For example, tf.argmax(y,1) is the label our model thinks is most likely for each input, while tf.argmax(y_,1) is the correct label. We can use tf.equal to check if our prediction matches the truth.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 13
        }, 
        {
            "source": "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": 14
        }, 
        {
            "source": "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))", 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0.9176\n"
                }
            ], 
            "execution_count": 15
        }, 
        {
            "source": "", 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "execution_count": null
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}